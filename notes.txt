
1 repeating path trap
https://www.ics.uci.edu/honors/advising/
redirects to 
https://www.ics.uci.edu/ugrad/honors/index.php/advising/

trap:
https://www.ics.uci.edu/ugrad/honors/index.php/*


examples:
https://www.ics.uci.edu/ugrad/honors/index.php/overview/advising/degrees/overview/degrees/advising/policies/overview/step-2-honors-seminar-ics-h197.php
https://www.ics.uci.edu/ugrad/honors/index.php/overview/advising/degrees/overview/degrees/advising/policies/policies/policies/policies/advising/computing/account/computing/sao/SAO_News_and_Updates.php
https://www.ics.uci.edu/ugrad/honors/index.php/overview/advising/degrees/overview/degrees/advising/policies/policies/policies/policies/advising/computing/account/computing/sao/sao/resources/policies/courses/forms.php
https://www.ics.uci.edu/ugrad/honors/index.php/overview/advising/degrees/overview/degrees/advising/policies/policies/policies/policies/advising/computing/account/computing/sao/sao/resources/policies/courses/policies/Course_Outside_UCI.php
https://www.ics.uci.edu/ugrad/honors/index.php/overview/advising/degrees/overview/degrees/advising/policies/policies/policies/policies/advising/computing/account/computing/sao/sao/resources/policies/courses/policies/resources/resources/degrees/resources/courses/overview/sao/policies/sao/resources/overview/resources/policies/degrees/policies/degrees/media.php
https://www.ics.uci.edu/ugrad/honors/index.php/overview/advising/degrees/overview/degrees/advising/policies/policies/policies/policies/advising/computing/account/computing/sao/sao/resources/policies/courses/policies/resources/resources/degrees/resources/courses/overview/sao/policies/sao/resources/overview/resources/policies/degrees/policies/degrees/sao/policies/forms.php


2. query trap
trap ex: https://swiki.ics.uci.edu/doku.php/network:pen?image=labs%3Adeploystudio_guide_2.0.pdf&tab_details=view&do=media&tab_files=upload&ns=group%3Asupport%3Aimaging_devices
sitemap:https://swiki.ics.uci.edu/doku.php/network:pen?do=index

solutions: 
    sort queries when crawling to make unique urls (since the order of the queries does not matter)
    check similarity of all pages with query. if there is not much difference between other query urls on page - blacklist all query urls on that webpage.

blacklist - 
    avoid bad links: 
        avoid some code - 404?
        avoid empty responses
    "low information value page":
        count frequency of tokens that are real words

url patterns:
    - ex: repeating directory/pattern
and/or text similarity - do not use links inside
                - compare to previous page

issue with same webpage with different query parameters
    ideas: sort query parameters
           limit number of times a url can be visited without the query


function - takes page url for each page
keep track of list of subdomains
    number of unique pages in each subdomain (ordered by freq / alpha )

global url variable - max page size (tokens)

function - takes page content for each page
50 most common words
    order by freq / alpha
    eliminate stop words





robots.txt - 
    blacklist disallowed
    sitemap - 
            crawl sitemap instead of domain if exists
            can use for freshness (not needed) 
                if the sitemap isnt kept up to date, check the actual website instead





multithread